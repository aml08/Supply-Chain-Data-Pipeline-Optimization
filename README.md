# üöõ Supply Chain Data Pipeline & SQL Optimization

![Python](https://img.shields.io/badge/Python-Pandas-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Optimization-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)
![ETL](https://img.shields.io/badge/Data_Engineering-ETL-green?style=for-the-badge)

## üìã Executive Summary
Ce projet vise √† optimiser la logistique de **KNAUF Industries** en centralisant des donn√©es h√©t√©rog√®nes dispers√©es.

L'objectif √©tait de construire une **Architecture Data Robuste** capable de traiter des milliers d'exp√©ditions pour analyser les co√ªts de transport, les retards et l'empreinte carbone.

### üéØ R√©sultats Cl√©s
* **Qualit√© de Donn√©es :** Sauvetage de **100%** du dataset critique via des strat√©gies d'imputation avanc√©es (M√©diane/Logique m√©tier).
* **Performance SQL :** R√©duction du temps d'ex√©cution des requ√™tes de **25%** (325ms $\to$ 244ms).
* **S√©curit√© :** Mise en place d'une gestion des acc√®s (RBAC) conforme aux principes de moindre privil√®ge.

---

## ‚öôÔ∏è Architecture Technique

### 1. Ingestion & ETL (Extract, Transform, Load)
Le d√©fi principal r√©sidait dans l'h√©t√©rog√©n√©it√© des sources de donn√©es :
* **Clients :** Format `JSON` (Semi-structur√©).
* **Exp√©ditions :** Format `CSV` (Plat).
* **Hubs Logistiques :** Format `Excel` (Propri√©taire).

> **Solution :** D√©veloppement d'un script Python (`pandas`) pour normaliser ces flux, g√©rer les encodages et typer les donn√©es avant l'insertion en base.

### 2. Data Cleaning & Qualit√©
Les donn√©es brutes (JSON, CSV, Excel) contenaient des incoh√©rences, notamment sur la localisation des clients.

| Colonne | Probl√®me | Strat√©gie de Nettoyage (Code Python) |
| :--- | :--- | :--- |
| `city` (Client) | Valeurs manquantes (NULL) pour le transporteur "MedLog" | **Imputation D√©ductive** (Correction bas√©e sur la logique m√©tier : MedLog est bas√© √† Marseille $\to$ `fillna`). |
| `Join Keys` | Donn√©es dispers√©es (Excel vs CSV) | **Unification des cl√©s** (`client_id`, `hub`) pour garantir la coh√©rence avant l'export SQL. |

### 3. Mod√©lisation & Base de Donn√©es (PostgreSQL)
Conception d'un sch√©ma en √©toile (Star Schema) pour faciliter les analyses BI :
* **Table de Fait :** `t_logistique` (Exp√©ditions, Co√ªts, D√©lais).
* **Dimensions :** `t_clients`, `t_hubs`.

---

## üöÄ Optimisation & Performance SQL

L'analyse des plans d'ex√©cution (`EXPLAIN ANALYZE`) a r√©v√©l√© des lenteurs sur les agr√©gations temporelles.

**Action :** Cr√©ation d'index cibl√©s sur les colonnes de filtrage fr√©quent.

```sql
-- Cr√©ation d'index pour acc√©l√©rer les recherches par date et code postal
CREATE INDEX idx_date_expedition ON t_logistique(date_expedition);
CREATE INDEX idx_code_postal ON t_clients(code_postal);

```

## ‚ö° Impact Mesur√©
* Temps sans index : 325 ms
* Temps avec index : 244 ms
* Gain de performance : ~25%

## üõ°Ô∏è S√©curit√© & Gouvernance (Security-Aware)
* En application des bonnes pratiques de cybers√©curit√© :

* Cr√©ation d'utilisateurs sp√©cifiques (analyst_logistique).

* Restriction des droits : Attribution stricte des privil√®ges SELECT uniquement sur les tables n√©cessaires, interdiction des commandes DROP ou ALTER pour les utilisateurs finaux.

```SQL
-- Exemple de gestion des privil√®ges (Principe du moindre privil√®ge)
CREATE USER analyst_logistique WITH PASSWORD 'secure_pass';
GRANT CONNECT ON DATABASE knauf_db TO analyst_logistique;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO analyst_logistique; 
```

## üõ†Ô∏è Comment utiliser ce projet ?
Cloner le repo :
```Bash
git clone https://github.com/aml08/Supply-Chain-Data-Pipeline-Optimization.git
```

Installer les d√©pendances :
```Bash
pip install pandas sqlalchemy psycopg2

```

Lancer le pipeline ETL :
```Bash
python data_cleaning_pipeline.py 

```

Projet r√©alis√© dans le cadre du Master Data - Validation des comp√©tences d'Architecture de Donn√©es.
